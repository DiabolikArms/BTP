{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import wget\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ChromeDriver\n",
    "service = Service('E:/Programs/Chrome Driver/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_into_view(driver, element):\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "    time.sleep(2)\n",
    "\n",
    "def click_element(driver, element):\n",
    "    try:\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(element).click().perform()\n",
    "    except Exception as e:\n",
    "        print(f\"Error interacting with element: {e}\")\n",
    "\n",
    "def click_show_more_comments(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            show_more_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[@id='comment-container-112968501']/div/div[1]/div[2]/div[1]/div/div/div[2]\"))\n",
    "            )\n",
    "            scroll_into_view(driver, show_more_button)\n",
    "            show_more_button.click()\n",
    "            time.sleep(2)\n",
    "        except (TimeoutException, ElementClickInterceptedException, StaleElementReferenceException) as e:\n",
    "            print(f\"Error with 'Show More Comments' button: {e}\")\n",
    "            break\n",
    "\n",
    "def extract_comments(driver, article_url, cat_tag):\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "\n",
    "        # Get the timestamp and author of the article\n",
    "        timestamp_element = driver.find_element(By.XPATH, \"//*[contains(concat(' ', @class, ' '), concat(' ', 'byline', ' '))]//span\")\n",
    "        timestamp = timestamp_element.text.strip()\n",
    "        try:\n",
    "            author_element = driver.find_element(By.XPATH, \"//*[contains(concat(' ', @class, ' '), ' mMwSH ')]\")\n",
    "            author = author_element.text\n",
    "        except NoSuchElementException:\n",
    "            author = \"TOI\"\n",
    "        article_div = driver.find_element(By.XPATH, \"//div[contains(@class, 'clearfix') and contains(@class, '_s30J')]\")\n",
    "        article_text = article_div.text.replace(\"\\n\", \" \").replace(\"\\'\", \"\")\n",
    "        \n",
    "        # Remove common prefixes like 'Updated:' if present\n",
    "        if \"Updated:\" in timestamp:\n",
    "            timestamp = timestamp.replace(\"Updated:\", \"\").strip()\n",
    "        \n",
    "        # Split the timestamp to remove the time part and clean up the string\n",
    "        try:\n",
    "            date_part = timestamp.split(\",\")[0] + \" \" + timestamp.split(\",\")[1].strip()  # Example: 'Sep 29 2024'\n",
    "            time_part = timestamp.split(\",\")[2].strip()  # Example: '04:58 IST'\n",
    "        \n",
    "            # Convert the date part to 'dd.mm.yyyy' format\n",
    "            date_object = datetime.strptime(date_part, \"%b %d %Y\")\n",
    "            formatted_date = date_object.strftime(\"%d.%m.%Y\")\n",
    "        \n",
    "            # Store the date and time separately\n",
    "            timestamp_date = formatted_date\n",
    "            timestamp_time = time_part\n",
    "        \n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Error processing timestamp '{timestamp}': {e}\")\n",
    "            timestamp_date = None\n",
    "            timestamp_time = None\n",
    "        \n",
    "        # Locate the \"End of Article\" element\n",
    "        end_of_article_div = driver.find_element(By.XPATH, \"//*[contains(concat(' ', @class, ' '), ' mj2wg ')]//span[contains(text(), 'End of Article')]\")\n",
    "        element_position = end_of_article_div.location['y']\n",
    "        viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "        scroll_position = element_position - (viewport_height / 2)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
    "        time.sleep(2)\n",
    "    except WebDriverException as e:\n",
    "        print(f\"Failed to load page {article_url}: {e}\")\n",
    "        return {}  # Return an empty list to skip this article\n",
    "    \n",
    "    # Prepare a list to store extracted comments data\n",
    "    comments_data = []\n",
    "    \n",
    "    try:\n",
    "        # Locate and click the comment button\n",
    "        try:\n",
    "            view_comment_btn = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//*[contains(concat(' ', @class, ' '), ' GzGIQ ') and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]\"))\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            view_comment_btn.click()\n",
    "        except TimeoutException as e:\n",
    "            print(f\"Timeout waiting for comment button: {e}\")\n",
    "            return {}\n",
    "        except ElementClickInterceptedException as e:\n",
    "            print(f\"Comment button click intercepted: {e}\")\n",
    "            return {}\n",
    "        \n",
    "        # Continuously check for \"Show More Comments\" button and click if found\n",
    "        while True:\n",
    "            try:\n",
    "                # Find the \"Show More Comments\" button\n",
    "                show_more_button = WebDriverWait(driver, 15).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//div[contains(concat(' ', @class, ' '), ' hduJ6 ') and contains(text(), 'View more comments')]\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button)\n",
    "                show_more_button.click()\n",
    "                time.sleep(2)  # Wait for new comments to load\n",
    "            except TimeoutException:\n",
    "                # No more \"Show More Comments\" button found, exit loop\n",
    "                print(\"Finished loading all the comments\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                # Button not found, break the loop\n",
    "                print(\"No 'Show More Comments' button found.\")\n",
    "                break\n",
    "            except ElementClickInterceptedException:\n",
    "                # Handle cases where the button is covered or not clickable\n",
    "                print(\"Failed to click 'Show More Comments' button due to an interception.\")\n",
    "                break\n",
    "        \n",
    "        # Locate and extract all comments\n",
    "        comments = driver.find_elements(By.XPATH, \"//*[contains(concat(' ', @class, ' '), concat(' ', 'Nuk1p', ' '))]\")\n",
    "        # Filter out any elements that contain an iframe (ads) or advertisement-specific divs\n",
    "        comments = [comment for comment in comments\n",
    "                    if len(comment.find_elements(By.XPATH, \".//div[contains(@class, 'paisa-wrapper')]\")) == 0]\n",
    "\n",
    "        for comment in comments:\n",
    "            try:\n",
    "                try:\n",
    "                    # Check if the \"Read More\" link exists \n",
    "                    read_more_link = comment.find_element(By.XPATH, \".//span[contains(text(), 'Read More')]\")\n",
    "                    driver.execute_script(\"arguments[0].click();\", read_more_link)\n",
    "                except NoSuchElementException:\n",
    "                    # If no \"Read More\" link is found\n",
    "                    pass\n",
    "                    \n",
    "                # Extract user and comment details\n",
    "                user = comment.find_element(By.CLASS_NAME, \"ZJ4ae\").text\n",
    "                comment_text = comment.find_element(By.CLASS_NAME, \"mxnGH\").text\n",
    "                upvotes = comment.find_element(By.XPATH, \".//*[contains(concat(' ', @class, ' '), ' LmcfZ ')]//span\").text\n",
    "                downvotes = comment.find_element(By.XPATH, \".//*[contains(concat(' ', @class, ' '), ' Qh8bj ')]//span\").text\n",
    "        \n",
    "                # Initialize replies list before processing\n",
    "                replies = []\n",
    "        \n",
    "                # Check and click \"Show responses\" link to load nested replies\n",
    "                try:\n",
    "                    show_responses_link = comment.find_element(By.XPATH, \"./div[4]/a\")\n",
    "                    scroll_into_view(driver, show_responses_link)\n",
    "                    show_responses_link.click()\n",
    "                    time.sleep(2)\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "                \n",
    "                # Locate nested replies after they are loaded\n",
    "                while True:\n",
    "                    try:\n",
    "                        show_all_responses_button = comment.find_element(By.XPATH, \".//div[contains(concat(' ', @class, ' '), ' evC4f ') and contains(text(), 'Show all responses')]\")\n",
    "                        show_all_responses_button.click()\n",
    "                        time.sleep(2)\n",
    "                    except NoSuchElementException:\n",
    "                        break  # Exit loop if no more \"Show all responses\" button is found\n",
    "                \n",
    "                reply_elements = comment.find_elements(By.XPATH, \".//div[5]/ul/li\")\n",
    "                reply_elements = [reply for reply in reply_elements\n",
    "                  if len(reply.find_elements(By.XPATH, \".//iframe\")) == 0 and\n",
    "                     len(reply.find_elements(By.XPATH, \".//div[contains(@class, 'paisa-wrapper')]\")) == 0]\n",
    "                \n",
    "                for reply in reply_elements:\n",
    "                    try:\n",
    "                        try:\n",
    "                            # Check if the \"Read More\" link exists \n",
    "                            read_more_link = reply.find_element(By.XPATH, \".//span[contains(text(), 'Read More')]\")\n",
    "                            driver.execute_script(\"arguments[0].click();\", read_more_link)\n",
    "                        except NoSuchElementException:\n",
    "                            # If no \"Read More\" link is found\n",
    "                            pass\n",
    "                            \n",
    "                        reply_user = reply.find_element(By.XPATH, \"./div[1]/h3\").text\n",
    "                        reply_to = reply.find_element(By.XPATH, \"./div[2]/span[1]\").text\n",
    "                        reply_text = reply.find_element(By.XPATH, \"./div[3]\").text\n",
    "                        reply_upvotes = reply.find_element(By.XPATH, \".//*[contains(concat(' ', @class, ' '), ' LmcfZ ')]//span\").text\n",
    "                        reply_downvotes = reply.find_element(By.XPATH, \".//*[contains(concat(' ', @class, ' '), ' Qh8bj ')]//span\").text\n",
    "                        \n",
    "                        replies.append({\n",
    "                            'user': reply_user,\n",
    "                            'reply_to': reply_to,\n",
    "                            'comment_text': reply_text,\n",
    "                            'upvotes': reply_upvotes,\n",
    "                            'downvotes': reply_downvotes,\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting reply comment: {e}\")\n",
    "                \n",
    "                # Append the extracted details to comments_data with article link\n",
    "                if user and upvotes.isdigit() and downvotes.isdigit():\n",
    "                    comments_data.append({\n",
    "                        'user': user,\n",
    "                        'comment_text': comment_text,\n",
    "                        'upvotes': upvotes,\n",
    "                        'downvotes': downvotes,\n",
    "                        'replies': replies\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipped an empty comment\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting comment: {e}\")\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(\"No comment button found, skipping this article.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking comment button: {e}\")\n",
    "    \n",
    "    article_data = {\n",
    "        'article_link': article_url,\n",
    "        'author': author,\n",
    "        'tag': cat_tag,\n",
    "        'date': timestamp_date,\n",
    "        'time': timestamp_time,\n",
    "        'article_content': article_text,\n",
    "        'comments': comments_data,\n",
    "    }\n",
    "    \n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all_article_hrefs is already defined and contains 28,000 elements\n",
    "recent_articles = []\n",
    "six_month_plus = []\n",
    "\n",
    "# Define the date range for the last 6 months\n",
    "six_months_ago = datetime.now() - timedelta(days=6*30)  # Roughly 6 months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk):\n",
    "    for link in chunk:\n",
    "        print(f\"Processing article: {link[0]}\")\n",
    "        article_data = extract_comments(driver, link[0], link[1])\n",
    "\n",
    "        try:\n",
    "            # Convert the article date string to a datetime object\n",
    "            article_date_str = article_data['date']  # Assuming 'date' is in \"dd.mm.yyyy\" format\n",
    "            article_date = datetime.strptime(article_date_str, \"%d.%m.%Y\")\n",
    "\n",
    "            # Check if the article date is within the last 6 months\n",
    "            if article_date >= six_months_ago:\n",
    "                recent_articles.append(article_data)\n",
    "                print(f\"Extracted {len(article_data['comments'])} comments\")\n",
    "            else:\n",
    "                six_month_plus.append(article_data)\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: 'date' not found in article data for {link[0]}, skipping this article.\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define the folder and file path for chunk 1\n",
    "folder_name = 'article_chunks'\n",
    "chunk_9_file = os.path.join(folder_name, 'article_hrefs_chunk_9.pkl')\n",
    "\n",
    "# Check if the file exists before loading\n",
    "if os.path.exists(chunk_9_file):\n",
    "    # Load chunk 1\n",
    "    with open(chunk_9_file, 'rb') as file:\n",
    "        chunk_9_data = pickle.load(file)\n",
    "    \n",
    "    # Use the chunk data (for example, print it)\n",
    "    print(\"Chunk 9 data:\", chunk_9_data)\n",
    "else:\n",
    "    print(f\"File {chunk_9_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_chunk(chunk_9_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total recent articles: {len(recent_articles)}\")\n",
    "print(f\"Total articles older than 6 months: {len(six_month_plus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recent9.pkl', 'wb') as recent_file:\n",
    "    pickle.dump(recent_articles, recent_file)\n",
    "    print(\"Saved recent articles to recent9.pkl\")\n",
    "\n",
    "# Save six months plus articles to six_plus1.pkl\n",
    "with open('six_plus9.pkl', 'wb') as six_month_file:\n",
    "    pickle.dump(six_month_plus, six_month_file)\n",
    "    print(\"Saved older articles to six_plus9.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
