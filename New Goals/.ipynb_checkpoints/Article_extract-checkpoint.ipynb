{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f73c5-cd6b-4753-a6a2-cf3f53ea66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import wget\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94da2b7-f1c6-4804-85cd-61d03ff50e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pair = []\n",
    "test_pair.append(('https://timesofindia.indiatimes.com/auto/motorsports/', 'AUTO'))\n",
    "test_pair.append(('https://timesofindia.indiatimes.com/life-style/health-fitness', 'LIFE STYLE'))\n",
    "# print(\"1 passed\")\n",
    "xpaths = [\n",
    "    \"//figcaption\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' sNF1c ')] | //h5\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' top-newslist ')]//a\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' top-newslist ')]//*[contains(concat(' ', @class, ' '), ' w_tle ')]//a\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' title ')]//span\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' WavNE ')]\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' desc ')]//a\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' leadstorties ')]//span\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' I4QgS ')]\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' null ')]//span | //figcaption | //*[contains(concat(' ', @class, ' '), ' linktype1 ')]//span\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' clearfix ')]//div//div//li//a\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' news-card-img ')]//*[contains(concat(' ', @class, ' '), ' w_tle ')]//a\",\n",
    "    \"//div[(count(preceding-sibling::*) + 1 = 8)]//*[contains(concat(' ', @class, ' '), ' M194D ')]//figcaption | //div[(count(preceding-sibling::*) + 1 = 8)]//*[contains(concat(' ', @class, ' '), ' hoid1 ')]//figcaption | //div[(count(preceding-sibling::*) + 1 = 9)]//figcaption | //div[(count(preceding-sibling::*) + 1 = 6)]//figcaption | //*[contains(concat(' ', @class, ' '), ' zxvyz ')]//figcaption | //*[contains(concat(' ', @class, ' '), ' sNF1c ')] | //div[(count(preceding-sibling::*) + 1 = 7)]//figcaption\",\n",
    "    \"//*[@id = 'c_headlines_wdt_1']//*[contains(concat(' ', @class, ' '), ' w_tle ')]//a\",\n",
    "    \"//*[@id = 'content']//*[contains(concat(' ', @class, ' '), ' w_tle ')]//a\",\n",
    "    \"//*[contains(concat(' ', @class, ' '), ' chng_lfttxt ')]//h3 | //*[contains(concat(' ', @class, ' '), ' mrB20 ')]//h3\",\n",
    "]\n",
    "\n",
    "# print(\"2 passed\")\n",
    "\n",
    "all_links = []\n",
    "\n",
    "for item in filtered_pairs:\n",
    "    webpage_link = item[0]\n",
    "    webpage_tag = item[1]\n",
    "    \n",
    "    # Navigate to the webpage\n",
    "    driver.get(webpage_link)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    captions = []  # Reset captions for each webpage\n",
    "    not_found_count = 0\n",
    "    \n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            # Find elements for the current XPath\n",
    "            elements = driver.find_elements(By.XPATH, xpath)\n",
    "            \n",
    "            # If elements are found, extend the captions list\n",
    "            if elements:\n",
    "                captions.extend(elements)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred for XPath {xpath}: {e}\")  # Log the error for debugging\n",
    "    print(len(captions))\n",
    "    # Process the captions if any are found\n",
    "    if captions:\n",
    "        hrefs = []\n",
    "\n",
    "        for caption in captions:\n",
    "                try:\n",
    "                    parent = caption\n",
    "                    anchor_found = False\n",
    "                    \n",
    "                    # Traverse up to 5 levels in the hierarchy to search for the <a> tag\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            # Try finding the <a> tag in the current parent level\n",
    "                            parent_a_tag = parent.find_element(By.XPATH, \"./parent::a\")\n",
    "                            \n",
    "                            # If found, extract the href and break the loop\n",
    "                            href = parent_a_tag.get_attribute(\"href\")\n",
    "                            if href:\n",
    "                                hrefs.append(href)\n",
    "                                print(f\"Current href count: {len(hrefs)}\")\n",
    "                                anchor_found = True\n",
    "                            # print(\"pass\")\n",
    "                        except:\n",
    "                            # Move to the next level in the hierarchy\n",
    "                            parent = parent.find_element(By.XPATH, \"./parent::*\")\n",
    "                    \n",
    "                    # Move to the next caption if anchor is not found\n",
    "                    if not anchor_found:\n",
    "                        not_found_count += 1\n",
    "\n",
    "                except StaleElementReferenceException:\n",
    "                    print(f\"StaleElementReferenceException encountered. Skipping this caption.\")\n",
    "                    break  # Exit loop after max retries\n",
    "                \n",
    "                except Exception as e:\n",
    "                    # Handle other unexpected exceptions\n",
    "                    print(f\"Error processing caption: {caption.text} - {e}\")\n",
    "\n",
    "        # Create pairs of webpage_tag and hrefs, and add them to the all_links list\n",
    "        for href in hrefs:\n",
    "            # print(f\"{href}\")\n",
    "            all_links.append((webpage_tag, href))\n",
    "\n",
    "    else:\n",
    "        print(f\"No captions found for {webpage_tag} ({webpage_link})\")\n",
    "\n",
    "    print(f\"Count of 'No <a> tag found within 5 levels' for category {webpage_tag}: {not_found_count}\")\n",
    "\n",
    "all_links = list(set(all_links))\n",
    "print(len(all_links))\n",
    "for pair in all_links:\n",
    "    print(f\"Category: {pair[0]}, Link: {pair[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
